{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformersNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 43.5/43.5 kB 708.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\python311\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\python311\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\python311\\lib\\site-packages (4.12.2)\n",
      "Collecting tweepy\n",
      "  Downloading tweepy-4.14.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in c:\\python311\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rohit\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.5/41.5 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python311\\lib\\site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rohit\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\python311\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\python311\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\python311\\lib\\site-packages (from tweepy) (1.3.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.24.0->transformers)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rohit\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/10.1 MB 3.3 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/10.1 MB 3.8 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.6/10.1 MB 5.0 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.1/10.1 MB 6.1 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.4/10.1 MB 6.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.0/10.1 MB 7.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.1/10.1 MB 7.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.1/10.1 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.2/10.1 MB 7.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.8/10.1 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.3/10.1 MB 8.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.7/10.1 MB 8.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.2/10.1 MB 8.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.4/10.1 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.2/10.1 MB 9.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.5/10.1 MB 9.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.0/10.1 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.3/10.1 MB 9.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.8/10.1 MB 8.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.2/10.1 MB 8.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.5/10.1 MB 8.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.9/10.1 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.3/10.1 MB 8.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.8/10.1 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.1/10.1 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 8.8 MB/s eta 0:00:00\n",
      "Downloading tweepy-4.14.0-py3-none-any.whl (98 kB)\n",
      "   ---------------------------------------- 0.0/98.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 98.5/98.5 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "   ---------------------------------------- 0.0/447.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 447.8/447.8 kB 9.3 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "   ---------------------------------------- 0.0/274.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 274.1/274.1 kB 8.5 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "   ---------------------------------------- 0.0/286.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 286.0/286.0 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.4 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.0/2.4 MB 10.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.2/2.4 MB 8.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.6/2.4 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.0/2.4 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.3/2.4 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 7.6 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "   ---------------------------------------- 0.0/179.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 179.6/179.6 kB 10.6 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, regex, fsspec, huggingface-hub, tweepy, tokenizers, transformers\n",
      "Successfully installed fsspec-2024.10.0 huggingface-hub-0.26.5 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.21.0 transformers-4.47.0 tweepy-4.14.0\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers pandas numpy requests beautifulsoup4 tweepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at yiyanghkust/finbert-tone.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"yiyanghkust/finbert-tone\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://finance.yahoo.com/news/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "headlines = [h.text for h in soup.find_all(\"h3\", class_=\"Mb(5px)\")]\n",
    "print(headlines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "# Authenticate with Twitter API\n",
    "consumer_key = \"xf8It8PJC7JUW0Qo3BZAnwk5o\"\n",
    "consumer_secret = \"8Esz5h7NqmnOGfJ3gRbTx3DWZOft8dATkeUpnrc30tNx3XG161\"\n",
    "access_token = \"1867573194926018561-GPnn4YzX6NMTSs4E6HPRo9TxMlqbRv\"\n",
    "access_token_secret = \"XYxUr7VRWKCk2M3wB149wPiKZ7kAJ0hgbEfOz7J59a0d3\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Search for tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TooManyRequests",
     "evalue": "429 Too Many Requests\nToo Many Requests",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m tweepy\u001b[38;5;241m.\u001b[39mClient(bearer_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAAAAAAAAAAAAAAAAAAAAALobxgEAAAAArG9itQF0SkffHZy2AEiIeGCcKAk\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m3DqTXhLQZQxJLq4rvKSStuX9HCEaKhEXltLBkbftoKgTZd36QO3o\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesla stock lang:en\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m tweets \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_recent_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tweets\u001b[38;5;241m.\u001b[39mdata:\n\u001b[0;32m      8\u001b[0m     tweet_texts \u001b[38;5;241m=\u001b[39m [tweet\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets\u001b[38;5;241m.\u001b[39mdata]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tweepy\\client.py:1266\u001b[0m, in \u001b[0;36mClient.search_recent_tweets\u001b[1;34m(self, query, user_auth, **params)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;124;03m\"\"\"search_recent_tweets( \\\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;124;03m    query, *, end_time=None, expansions=None, max_results=None, \\\u001b[39;00m\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;124;03m    media_fields=None, next_token=None, place_fields=None, \\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;124;03m.. _Academic Research Project: https://developer.twitter.com/en/docs/projects\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1265\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m query\n\u001b[1;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/2/tweets/search/recent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexpansions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedia.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnext_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplace.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpoll.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msince_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msort_order\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtweet.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muntil_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTweet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_auth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_auth\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tweepy\\client.py:129\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[1;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, route, params\u001b[38;5;241m=\u001b[39m{}, endpoint_parameters\u001b[38;5;241m=\u001b[39m(), json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m     data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    126\u001b[0m ):\n\u001b[0;32m    127\u001b[0m     request_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_params(params, endpoint_parameters)\n\u001b[1;32m--> 129\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_auth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_auth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_type \u001b[38;5;129;01mis\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tweepy\\client.py:115\u001b[0m, in \u001b[0;36mBaseClient.request\u001b[1;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(method, route, params, json, user_auth)\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests(response)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TwitterServerError(response)\n",
      "\u001b[1;31mTooManyRequests\u001b[0m: 429 Too Many Requests\nToo Many Requests"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "client = tweepy.Client(bearer_token=\"AAAAAAAAAAAAAAAAAAAAALobxgEAAAAArG9itQF0SkffHZy2AEiIeGCcKAk%3DqTXhLQZQxJLq4rvKSStuX9HCEaKhEXltLBkbftoKgTZd36QO3o\")\n",
    "\n",
    "query = \"Tesla stock lang:en\"\n",
    "tweets = client.search_recent_tweets(query=query, max_results=100)\n",
    "if tweets.data:\n",
    "    tweet_texts = [tweet.text for tweet in tweets.data]\n",
    "    for tweet in tweets.data:\n",
    "        print(tweet.text)\n",
    "else:\n",
    "    print(\"No tweets found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^A-Za-z\\s]\", \"\", text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "cleaned_data = [clean_text(headline) for headline in headlines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sentiment_analyzer(cleaned_data)\n",
    "for text, result in zip(cleaned_data, results):\n",
    "    print(f\"Text: {text}\\nSentiment: {result['label']} (Score: {result['score']})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "scores = [result[\"score\"] if result[\"label\"] == \"positive\" else -result[\"score\"] for result in results]\n",
    "overall_sentiment = np.mean(scores)\n",
    "print(f\"Overall Sentiment Score: {overall_sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"Text\": cleaned_data, \"Sentiment\": [res[\"label\"] for res in results], \"Score\": scores})\n",
    "df.to_csv(\"sentiment_analysis_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(scores)), scores, marker=\"o\")\n",
    "plt.title(\"Sentiment Trend\")\n",
    "plt.xlabel(\"Data Point Index\")\n",
    "plt.ylabel(\"Sentiment Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting prawcore<3,>=2.4 (from praw)\n",
      "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update_checker>=0.18 (from praw)\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\python311\\lib\\site-packages (from praw) (1.5.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\python311\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2022.12.7)\n",
      "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
      "   ---------------------------------------- 0.0/189.3 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 30.7/189.3 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 92.2/189.3 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 184.3/189.3 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 189.3/189.3 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: update_checker, prawcore, praw\n",
      "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at yiyanghkust/finbert-tone were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at yiyanghkust/finbert-tone.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for Tesla...\n",
      "Fetching Reddit posts for Tesla...\n",
      "Warning: Some texts exceed the maximum length (3).\n",
      "Tesla Sentiment Score: -0.9383600646594785\n",
      "Fetching market-wide news...\n",
      "Fetching market-wide Reddit posts...\n",
      "Warning: Some texts exceed the maximum length (21).\n",
      "Market Sentiment Score: -0.9371528248243695\n",
      "\n",
      "Summary:\n",
      "Sentiment Score for Tesla: -0.9383600646594785\n",
      "Overall Market Sentiment Score: -0.9371528248243695\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "import praw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize LLM\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"yiyanghkust/finbert-tone\")\n",
    "\n",
    "# Reddit API credentials (replace with your own credentials)\n",
    "REDDIT_CLIENT_ID = \"uq0b4nrBAwsxZzkVO8jNDQ\"\n",
    "REDDIT_SECRET = \"RseZDvyOlE8s1YN_q2FEL-8d1W14Mg\"\n",
    "REDDIT_USER_AGENT = \"test\"\n",
    "REDDIT_USERNAME = \"Clean_Assistance5401\"\n",
    "REDDIT_PASSWORD = \"Boruto@2023\"\n",
    "\n",
    "# Authenticate Reddit\n",
    "reddit = praw.Reddit(client_id=REDDIT_CLIENT_ID,\n",
    "                     client_secret=REDDIT_SECRET,\n",
    "                     user_agent=REDDIT_USER_AGENT,\n",
    "                     username=REDDIT_USERNAME,\n",
    "                     password=REDDIT_PASSWORD)\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^A-Za-z\\s]\", \"\", text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Truncate text function\n",
    "def truncate_text(text, max_length=480):\n",
    "    tokens = text.split()\n",
    "    if len(tokens) > max_length:\n",
    "        text = ' '.join(tokens[:max_length])\n",
    "    return text\n",
    "\n",
    "# Function to fetch stock-specific news\n",
    "def fetch_stock_news(stock_name):\n",
    "    url = f\"https://www.google.com/search?q={stock_name}+stock+news\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    headlines = [h.text for h in soup.find_all(\"h3\")]\n",
    "    return headlines\n",
    "\n",
    "# Function to fetch market-wide news\n",
    "def fetch_market_news():\n",
    "    url = \"https://finance.yahoo.com/news/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    headlines = [h.text for h in soup.find_all(\"h3\", class_=\"Mb(5px)\")]\n",
    "    return headlines\n",
    "\n",
    "    cleaned_data = [truncate_text(clean_text(text), max_length=512) for text in data]\n",
    "def analyze_sentiment(data):\n",
    "    cleaned_data = [truncate_text(clean_text(text),max_length=520) for text in data]\n",
    "    long_texts = [text for text in cleaned_data if len(text) > 512]\n",
    "    if long_texts:\n",
    "        print(f\"Warning: Some texts exceed the maximum length ({len(long_texts)}).\")\n",
    "    # Truncate remaining texts\n",
    "    cleaned_data = [text[:512] for text in cleaned_data if len(text) <= 512]\n",
    "    results = sentiment_analyzer(cleaned_data)\n",
    "    scores = [\n",
    "        result[\"score\"] if result[\"label\"] == \"positive\" else -result[\"score\"]\n",
    "        for result in results\n",
    "    ]\n",
    "    return np.mean(scores), pd.DataFrame({\n",
    "        \"Text\": cleaned_data,\n",
    "        \"Sentiment\": [res[\"label\"] for res in results],\n",
    "        \"Score\": scores,\n",
    "    })\n",
    "\n",
    "# Dynamic sentiment function\n",
    "def get_sentiment(stock_name=None):\n",
    "    if stock_name:\n",
    "        # Fetch stock-specific news\n",
    "        print(f\"Fetching news for {stock_name}...\")\n",
    "        stock_news = fetch_stock_news(stock_name)\n",
    "\n",
    "        # Fetch stock-specific Reddit posts\n",
    "        print(f\"Fetching Reddit posts for {stock_name}...\")\n",
    "        stock_posts = [post.title + \" \" + post.selftext for post in reddit.subreddit('all').search(stock_name, limit=100)]\n",
    "\n",
    "        # Combine news and Reddit posts\n",
    "        stock_data = stock_news + stock_posts\n",
    "        stock_sentiment, stock_df = analyze_sentiment(stock_data)\n",
    "        stock_df.to_csv(f\"{stock_name}_sentiment.csv\", index=False)\n",
    "        print(f\"{stock_name} Sentiment Score: {stock_sentiment}\")\n",
    "    else:\n",
    "        stock_sentiment = None\n",
    "\n",
    "    # Fetch market-wide data\n",
    "    print(\"Fetching market-wide news...\")\n",
    "    market_news = fetch_market_news()\n",
    "    print(\"Fetching market-wide Reddit posts...\")\n",
    "    market_posts = [post.title + \" \" + post.selftext for post in reddit.subreddit('all').search(\"stock market\", limit=100)]\n",
    "\n",
    "    # Combine market-wide data\n",
    "    market_data = market_news + market_posts\n",
    "    market_sentiment, market_df = analyze_sentiment(market_data)\n",
    "    market_df.to_csv(\"market_sentiment.csv\", index=False)\n",
    "    print(f\"Market Sentiment Score: {market_sentiment}\")\n",
    "\n",
    "    return stock_sentiment, market_sentiment\n",
    "\n",
    "# Run the analysis dynamically\n",
    "if __name__ == \"__main__\":\n",
    "    stock_name = input(\"Enter a stock name (or press Enter to skip): \").strip()\n",
    "    stock_sentiment, market_sentiment = get_sentiment(stock_name if stock_name else None)\n",
    "\n",
    "    print(\"\\nSummary:\")\n",
    "    if stock_sentiment is not None:\n",
    "        print(f\"Sentiment Score for {stock_name}: {stock_sentiment}\")\n",
    "    print(f\"Overall Market Sentiment Score: {market_sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask-cors\n",
      "  Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: Flask>=0.9 in c:\\python311\\lib\\site-packages (from flask-cors) (3.0.3)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\python311\\lib\\site-packages (from Flask>=0.9->flask-cors) (3.0.2)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\python311\\lib\\site-packages (from Flask>=0.9->flask-cors) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\python311\\lib\\site-packages (from Flask>=0.9->flask-cors) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\rohit\\appdata\\roaming\\python\\python311\\site-packages (from Flask>=0.9->flask-cors) (8.1.3)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\rohit\\appdata\\roaming\\python\\python311\\site-packages (from Flask>=0.9->flask-cors) (1.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\rohit\\appdata\\roaming\\python\\python311\\site-packages (from click>=8.1.3->Flask>=0.9->flask-cors) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python311\\lib\\site-packages (from Jinja2>=3.1.2->Flask>=0.9->flask-cors) (2.1.2)\n",
      "Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl (14 kB)\n",
      "Installing collected packages: flask-cors\n",
      "Successfully installed flask-cors-5.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install flask-cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at yiyanghkust/finbert-tone were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at yiyanghkust/finbert-tone.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [14/Dec/2024 03:41:34] \"POST /sentiment%20\\\\%20-H%20\"Content-Type:%20application/json\"%20\\\\%20-d%20'{\"stock\":%20\"AAPL\"}' HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [14/Dec/2024 03:43:06] \"POST /sentiment%20-H%20\"Content-Type:%20application/json\"%20-d%20'{\"stock\":%20\"AAPL\"}' HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [14/Dec/2024 03:44:37] \"POST /sentiment%20-H%20\"Content-Type:%20application/json\"%20-d%20'{\"stock\":%20\"AAPL\"}' HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import praw\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Enable Cross-Origin Resource Sharing for API integration\n",
    "\n",
    "# Initialize Sentiment Analyzer\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"yiyanghkust/finbert-tone\")\n",
    "\n",
    "# Reddit API credentials (replace with your credentials)\n",
    "REDDIT_CLIENT_ID = \"REDDIT_CLIENT_ID\"\n",
    "REDDIT_SECRET = \"REDDIT_SECRET\"\n",
    "REDDIT_USER_AGENT = \"REDDIT_USER AGENT\"\n",
    "REDDIT_USERNAME = \"YOUR_REDDIT_USERNAME\"\n",
    "REDDIT_PASSWORD = \"YOUR_REDDIT_PASSWORD\"\n",
    "\n",
    "# Authenticate Reddit\n",
    "reddit = praw.Reddit(\n",
    "    client_id=REDDIT_CLIENT_ID,\n",
    "    client_secret=REDDIT_SECRET,\n",
    "    user_agent=REDDIT_USER_AGENT,\n",
    "    username=REDDIT_USERNAME,\n",
    "    password=REDDIT_PASSWORD,\n",
    ")\n",
    "\n",
    "# Helper Functions\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^A-Za-z\\s]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def truncate_text(text, max_length=512):\n",
    "    tokens = text.split()\n",
    "    if len(tokens) > max_length:\n",
    "        text = ' '.join(tokens[:max_length])\n",
    "    return text\n",
    "\n",
    "def analyze_sentiment(data):\n",
    "    cleaned_data = [truncate_text(clean_text(text)) for text in data]\n",
    "    long_texts = [text for text in cleaned_data if len(text) > 512]\n",
    "    if long_texts:\n",
    "        print(f\"Warning: Some texts exceed the maximum length ({len(long_texts)}).\")\n",
    "    # Truncate remaining texts\n",
    "    cleaned_data = [text[:512] for text in cleaned_data if len(text) <= 512]\n",
    "    results = sentiment_analyzer(cleaned_data)\n",
    "    scores = [\n",
    "        result[\"score\"] if result[\"label\"] == \"positive\" else -result[\"score\"]\n",
    "        for result in results\n",
    "    ]\n",
    "    avg_score = np.mean(scores)\n",
    "    sentiment = \"positive\" if avg_score > 0 else \"negative\" if avg_score < 0 else \"neutral\"\n",
    "    return sentiment, avg_score\n",
    "\n",
    "@app.route(\"/sentiment\", methods=[\"POST\"])\n",
    "def get_sentiment():\n",
    "    stock_name = request.json.get(\"stock\")\n",
    "    print(stock_name)\n",
    "    if not stock_name:\n",
    "        return jsonify({\"error\": \"Stock name is required\"}), 400\n",
    "\n",
    "    # Fetch stock-specific Reddit posts\n",
    "    stock_posts = [post.title + \" \" + post.selftext for post in reddit.subreddit(\"all\").search(stock_name, limit=50)]\n",
    "\n",
    "    # Analyze Sentiment\n",
    "    sentiment, avg_score = analyze_sentiment(stock_posts)\n",
    "\n",
    "    print(stock_name,' ',sentiment,' ',avg_score)\n",
    "\n",
    "    return jsonify({\n",
    "        \"stock\": stock_name,\n",
    "        \"sentiment\": sentiment,\n",
    "        \"score\": avg_score\n",
    "    })\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
